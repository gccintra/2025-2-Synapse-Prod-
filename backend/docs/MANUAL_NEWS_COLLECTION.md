# Manual de Execu√ß√£o da Coleta de Not√≠cias

## Introdu√ß√£o

O sistema de coleta de not√≠cias usa uma abordagem **simplificada e confi√°vel** que pode ser executada manualmente a qualquer momento. √â ideal para:

- üß™ **Testes e debugging** do sistema de coleta
- üîÑ **Execu√ß√£o sob demanda** quando necess√°rio
- üìä **Monitoramento de funcionamento** do sistema
- üõ†Ô∏è **Troubleshooting** de problemas

## Requisitos

- Python 3.12+
- Docker containers rodando (banco de dados)
- Vari√°veis de ambiente configuradas:
  - `GNEWS_API_KEY` - Chave da API GNews
  - `DATABASE_URL` - URL do banco PostgreSQL
  - `GEMINI_API_KEY` - Chave da API Google Gemini (para keywords)

## Sistema Atual (Simplificado)

O sistema atual usa **apenas uma abordagem** de coleta:

### collect_news_simple()
1. **Busca todos os t√≥picos ativos** no banco de dados
2. **Gera keywords em batch** usando Gemini AI (1 chamada)
3. **Para cada t√≥pico**: busca not√≠cias no GNews + salva no banco
4. **Sem cache, sem prioriza√ß√£o, sem configura√ß√µes complexas**

---

## Execu√ß√£o Manual

### Comando Principal

```bash
# Dentro do container Docker (recomendado)
docker exec synapse-backend python /app/backend/app/jobs/collect_news.py

# Ou via module
docker exec synapse-backend python -m app.jobs.collect_news

# Execu√ß√£o local (se ambiente configurado)
cd backend
python -m app.jobs.collect_news
```

### Verifica√ß√£o R√°pida

```bash
# Verificar se container est√° rodando
docker ps | grep synapse-backend

# Verificar logs em tempo real
docker logs -f synapse-backend

# Testar conectividade
docker exec synapse-backend python -c "
from app.repositories.topic_repository import TopicRepository
print('Topics ativos:', len(TopicRepository().get_all_active()))
"
```

---

## Sa√≠da do Sistema

### 1. Logs de In√≠cio

```
================================================================================
JOB DE COLETA SIMPLIFICADA INICIADO
================================================================================
AIService inicializado com sucesso (modelo=gemini-2.5-flash, timeout=60s).
KeywordGenerationService inicializado
```

### 2. Processamento por Etapas

```
================================================================================
INICIANDO COLETA SIMPLIFICADA DE NOT√çCIAS
================================================================================
[1/3] Buscando t√≥picos ativos do banco de dados...
Encontrados 8 t√≥picos ativos: ['technology', 'politics', 'business', ...]

[2/3] Coletando not√≠cias para 8 t√≥picos...
    Gerando keywords para todos os t√≥picos em batch...
Keywords geradas para 8 t√≥picos

  [1/8] Buscando not√≠cias para o t√≥pico: 'technology' (ID=1)
    Query para 'technology': "Apple iPhone" OR "Google AI" OR "Microsoft Windows"
GNews retornou 10 artigos
    Search encontrou: 10 artigos

  [2/8] Buscando not√≠cias para o t√≥pico: 'politics' (ID=2)
    Query para 'politics': "Joe Biden" OR "Donald Trump" OR "US Congress"
...
```

### 3. Processamento de Artigos

```
[3/3] Processando e salvando not√≠cias...
  Processando 10 artigos do t√≥pico 'technology'...
    Not√≠cia salva: 'Apple iPhone 17 Pro vs Apple iPhone 16...' ‚Üí t√≥pico 'technology' (ID=1)
    Not√≠cia salva: 'Wikipedia blames ChatGPT for falling traffic...' ‚Üí t√≥pico 'technology' (ID=1)
‚ö†Ô∏è  DOM√çNIO BLOQUEADO AUTOMATICAMENTE: 'bloomberg.com' (erro: 403 Forbidden)
...
```

### 4. Resumo Final

```
================================================================================
COLETA SIMPLIFICADA FINALIZADA!
RESUMO:
  - T√≥picos processados: 8 (do banco de dados)
  - Estrat√©gia: 1 busca por t√≥pico com keywords de IA em batch
  - Chamadas GNews: 8
  - Artigos coletados: 80
  - Novos artigos salvos: 57
  - Novas fontes: 13
================================================================================
JOB FINALIZADO COM SUCESSO
Resumo: 57 not√≠cias e 13 fontes salvas
```

---

## Consumo de APIs

### Por Execu√ß√£o (8 t√≥picos ativos):

| API | Chamadas | Uso do Limite | Observa√ß√µes |
|-----|----------|---------------|-------------|
| **GNews** | 8 | ~8% do limite di√°rio | 1 call por t√≥pico ativo |
| **Gemini** | 1 | ~0.5% do limite di√°rio | Batch keyword generation |

### Rate Limiting Autom√°tico:
- **GNews**: 2 segundos entre chamadas
- **Gemini**: 60 segundos timeout
- **Retry**: Autom√°tico para erros 429

---

## Troubleshooting

### ‚ùå Erro: GNEWS_API_KEY n√£o configurada

```
KeyError: 'GNEWS_API_KEY'
```

**Solu√ß√£o:** Configure no arquivo `.env`:
```bash
GNEWS_API_KEY=sua_chave_aqui
```

### ‚ùå Erro: Limite de API atingido (429)

```
HTTPError: 429 Client Error: Too Many Requests
```

**Solu√ß√µes:**
- Aguarde alguns minutos antes de tentar novamente
- Verifique seu plano da API GNews (100 calls/day gratuito)
- Monitor de uso: https://gnews.io/dashboard

### ‚ùå Erro: Banco de dados n√£o acess√≠vel

```
sqlalchemy.exc.OperationalError: could not connect to server
```

**Solu√ß√£o:**
```bash
# Verificar containers
docker compose ps

# Reiniciar se necess√°rio
docker compose down
docker compose up -d

# Verificar DATABASE_URL no .env
```

### ‚ùå Erro: Gemini API falhando

```
GoogleAPIError: 403 Forbidden
```

**Solu√ß√µes:**
- Verifique `GEMINI_API_KEY` no `.env`
- Confirme que API est√° ativa no Google Cloud Console
- Verifique cotas/billing no projeto

### ‚ö†Ô∏è Poucos artigos coletados

**Causas poss√≠veis:**
1. **Artigos duplicados**: URLs j√° existem no banco
2. **Sites bloqueados**: Muitos dom√≠nios na blacklist
3. **Keywords muito espec√≠ficas**: IA gerou termos muito restritivos

**Debugging:**
```bash
# Ver blacklist atual
docker exec synapse-backend cat /tmp/scraping_blacklist.json | jq '.blocked_domains | length'

# Ver dom√≠nios bloqueados
docker exec synapse-backend cat /tmp/scraping_blacklist.json | jq '.blocked_domains | keys'

# Contar not√≠cias no banco
docker exec synapse-backend python -c "
from app.repositories.news_repository import NewsRepository
print('Total de not√≠cias:', NewsRepository().count_all())
"
```

### üìä Sistema muito lento

**Otimiza√ß√µes:**
1. **Blacklist crescendo**: Sites problem√°ticos s√£o bloqueados automaticamente
2. **Rate limiting**: 2s por chamada GNews √© necess√°rio
3. **Scraping timeout**: Sites lentos s√£o abandonados ap√≥s 30s

**Tempo esperado**: 2-4 minutos para 8 t√≥picos

---

## Verifica√ß√£o P√≥s-Execu√ß√£o

### 1. Verificar no Banco de Dados

```sql
-- √öltimas not√≠cias coletadas
SELECT title, created_at, source_id
FROM news
ORDER BY created_at DESC
LIMIT 10;

-- Contagem por t√≥pico
SELECT t.name, COUNT(n.id) as news_count
FROM topics t
LEFT JOIN news n ON t.id = n.topic_id
GROUP BY t.name
ORDER BY news_count DESC;

-- Not√≠cias das √∫ltimas 24h
SELECT COUNT(*) FROM news
WHERE created_at > NOW() - INTERVAL '24 hours';
```

### 2. Testar APIs

```bash
# Testar endpoint de not√≠cias
curl http://localhost:5001/api/news | jq '.news | length'

# Testar feed personalizado
curl -H "Authorization: Bearer SEU_TOKEN" \
     http://localhost:5001/api/news/for-you | jq '.news | length'

# Verificar t√≥picos
curl http://localhost:5001/api/topics | jq '.topics | length'
```

### 3. Verificar Frontend

```bash
# Abrir no navegador
open http://localhost:5173

# Ou testar conectividade
curl -I http://localhost:5173
```

---

## Configura√ß√µes do Sistema

### Par√¢metros Hardcoded

O sistema atual usa valores fixos (sem arquivo de config):

```python
# GNews Search Parameters
search_params = {
    'lang': 'en',        # Ingl√™s fixo
    'country': 'us',     # Estados Unidos fixo
    'max': 10,           # 10 artigos por t√≥pico
}

# Rate Limiting
GNEWS_DELAY = 2          # 2 segundos entre calls
SCRAPING_TIMEOUT = 30    # 30s timeout para scraping
GEMINI_TIMEOUT = 60      # 60s timeout para IA
```

### Modificar Comportamento

Para alterar par√¢metros, edite diretamente:
- **NewsCollectService**: `backend/app/services/news_collect_service.py`
- **Palavras-chave**: Prompt no `KeywordGenerationService`
- **Rate limiting**: `backend/app/utils/api_rate_limiter.py`

---

## Dados Gerados

### Blacklist Autom√°tica

**Localiza√ß√£o**: `/tmp/scraping_blacklist.json`

```bash
# Ver blacklist
docker exec synapse-backend cat /tmp/scraping_blacklist.json | jq

# Limpar blacklist (se necess√°rio)
docker exec synapse-backend rm /tmp/scraping_blacklist.json
```

### Logs Persistentes

```bash
# Ver logs hist√≥ricos
docker logs synapse-backend | grep "JOB DE COLETA" -A 20

# Logs em tempo real
docker logs -f synapse-backend
```

---

## Pr√≥ximos Passos

### Automatiza√ß√£o

Para execu√ß√£o autom√°tica, configure cron:

```bash
# Editar crontab
crontab -e

# Executar a cada 6 horas
0 */6 * * * docker exec synapse-backend python /app/backend/app/jobs/collect_news.py
```

### Monitoramento

Monitore execu√ß√µes via:
1. **Logs Docker**: Sucesso/falhas da execu√ß√£o
2. **Banco de dados**: Volume de not√≠cias coletadas
3. **Blacklist**: Crescimento de dom√≠nios bloqueados

### Melhorias Futuras

1. **Detec√ß√£o de duplicatas por t√≠tulo** (planejado)
2. **M√∫ltiplas fontes** al√©m do GNews
3. **Configura√ß√µes via vari√°veis de ambiente**
4. **Dashboard de monitoramento**

---

## Refer√™ncias

- [Sistema de Coleta - Documenta√ß√£o Principal](../SISTEMA_COLETA_NOTICIAS.md)
- [GNews API Documentation](https://gnews.io/docs/v4)
- [NewsCollectService](../app/services/news_collect_service.py)
- [Job Principal](../app/jobs/collect_news.py)

---

**√öltima atualiza√ß√£o**: 2025-10-21
**Vers√£o**: Sistema Simplificado v2.0